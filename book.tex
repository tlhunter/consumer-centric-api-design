\documentclass{book}
\setcounter{tocdepth}{2}
\setcounter{secnumdepth}{3}
\title{A Consumer-Centric Approach to RESTful API Design}
\author{Thomas Hunter II}
\date{January 2014}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{epstopdf}
\usepackage{wallpaper}
\usepackage{geometry}
\renewcommand\bibname{References}

\begin{document}

\newgeometry{left=1cm,bottom=3cm,right=1cm,top=3cm}
\ThisCenterWallPaper{0.7}{images/cover.eps}
\thispagestyle{empty}
\begin{center}

\LARGE{\textbf{A Consumer-Centric Approach to RESTful API Design}}

\null
\vfill

\Large{Thomas Hunter II}

\large{January 2014}

\end{center}

\newpage
\restoregeometry

\section*{The People behind the Book}

\subsubsection*{Thomas Hunter II, Author}

\href{http://thomashunter.name}{Thomas} previously worked as a Developer Advocate and API Architect of a large filesharing and storage service, where his main concern was getting a well-documented API into the hands of third-party developers. While working at a company in the financial industry he evangelized an internal API facade to abstract backend disparity. He's also given various talks on the topic of API Design. His previous book \href{http://amzn.to/1aqVRvq}{Backbone.js Application Development} guides the reader through the process of building an API-consuming JavaScript application.

\subsubsection*{John Sheehan, Technical Reviewer}

John is an API fanatic with over 15 years of experience working in a wide variety of IT and software development roles. As an early employee at Twilio, John lead the developer evangelism program and worked as a Product Manager for Developer Experience. After Twilio, John was Platform Lead at IFTTT working with API providers to create new channels. John is also the creator of \href{https://www.github.com/restsharp/restsharp}{RestSharp}, \href{http://www.apidigest.com/}{API Digest}, \href{http://www.api-jobs.com/}{API Jobs} and co-host of \href{http://trafficandweather.io/}{Traffic and Weather}, an API and cloud podcast.

\subsubsection*{Kevin Swiber, Technical Reviewer}

Kevin is a senior software developer and Enterprise Architect with experience in multiple languages, platforms, and operating systems. Currently focused on cloud-based solutions and scalable, distributed systems. Leading the charge for open source development and inter-platform collaboration in the greater software development community.

TODO: Get Updated Second-Person Biography from Kevin

\subsubsection*{Jason Marciak, Editor}

TODO: Get Biography from Jason

\subsubsection*{Jon Kuperman, Influence}

I'd like to make a special thanks to \href{https://twitter.com/thecodeplanet}{Jon}, who once told me "Tom, you should turn that \href{http://codeplanet.io/principles-good-restful-api-design/}{API blog post} into a book."

\newpage

\section*{Introduction}

An API represents a contract between the data and business-logic stored on your Server, and Consumers who wish to interact with this data. Breaking this contract will result in angry emails from developers and anguished users with broken apps. Designing a hard-to-use API will result in few or no third-party developers using it. On the other hand, building a great API and adhering to this contract will result in substantially more third-party developers and can elevate your service from a closed-source product to an open platform.

Building an API is one of the most important things you can do to increase the value of your service. By having an API, your service/application has the potential to become a platform from which other services grow. Look at these tech companies offering popular services: \href{https://developers.facebook.com/}{Facebook}, \href{https://dev.twitter.com/docs/api/1.1}{Twitter}, \href{https://developers.google.com/}{Google}, \href{http://developer.github.com/v3/}{GitHub}, \href{https://developer.amazon.com/}{Amazon}, \href{http://developer.netflix.com/}{Netflix}, \href{https://www.dropbox.com/developers/core/docs}{Dropbox}... None of them would be nearly as popular as they are today if they hadn't opened their data and functionality via API. An entire industry exists with the sole purpose of consuming data provided by these platforms.

The principles of this book, if followed while designing and maintaining your API, will ensure third-party developers grok your API while substantially reducing support tickets and confused emails. Developers will travel from tech conference to tech conference rejoicing what a pleasure working with your API has been, getting more developers to sign-up along the way.

\subsection*{Approach}

This book will take a language-agnostic approach to demonstrating good API design. While there will be a few code examples here and there, you won't actually need to run any of them to understand what is happening. In fact, many of the topics are more philosophical than technical, and this book will make a great candidate for sitting on the back of your toilet.

\subsection*{Intended Audience}

The ideal reader of this book is someone who has already built several websites and is comfortable working with a web language or framework, as well as having some intermediate knowledge such as how to read and write HTTP headers.

While knowledge of any particular language is not a requirement of this book, a basic understanding of SQL will be beneficial for understanding the example queries.

\subsection*{Goals}

By the time you're done reading this book you'll have a high-level understanding of how to build a RESTful API Ecosystem which third-party developers will love. This book will not cover the technical details of how to program an API from scratch.


\tableofcontents

\newpage


\chapter{The Basics}

\section{Data Design and Abstraction}

Designing a \href{http://en.wikipedia.org/wiki/Restful}{RESTful API} means abstracting the intricate business-logic and data your service uses into the four basic CRUD concepts (\emph{Create}, \emph{Read}, \emph{Update}, \emph{Delete}). Your application may perform many complex actions behind the scenes such as sending a text message or resizing an image or moving a file, but if you do enough planning and abstraction, everything can be represented as CRUD.

Architecting your API begins earlier than you may think; first you need to decide how your data will be stored and how your service/application functions. If you're practicing \href{http://blog.pop.co/post/67465239611/why-we-chose-api-first-development}{API-First Development} \cite{APIFIRST} this is all part of the process. However, if you're bolting an API onto an existing project, you will have more abstraction to take care of.

In an idealized, overly-simple service, a Collection can represent a database table, and a Resource can represent a row within that table. As real-world practice will show this is rarely the case, especially if the existing data design is overly complex. It is important that you don't overwhelm third-party developers with complex application data, otherwise they won't want to use your API.

There will be parts of your service which you \emph{should not} expose via API at all. A common example is that many APIs will not allow Consumers to create or delete user accounts, or data shared between many accounts.

Sometimes multiple tables will be represented as a single resource (\texttt{JOIN} statements come in handy here). You might even find that one table should have multiple resources (although, you may have made some poor database design decisions if this is the case).

\subsection{Examples of Abstraction}

For both of these examples of abstraction, we'll make use of the same fictional service. This service sends messages to different users, and messages can either be sent as a text message or an email. The chosen method depends on the preference of the particular user.

Don't worry too much about the technical parts of the examples as we'll cover them with more detail in a later chapter. For now, just think of them as simple function calls with inputs and outputs.

\subsubsection{Good Abstraction}

Here's a clean and simple RESTful approach for sending a notification.

\paragraph{\textbf{Creating a Notification}}

\begin{verbatim}
POST /notifications

{
  "user_id": "12",
  "message": "Hello World"
}

{
  "id": "1000",
  "user_id": "12",
  "message": "Hello World",
  "medium": "email",
  "created": "2013-01-06 21:02:00"
}
\end{verbatim}

In this example we use a single endpoint for sending a notification to a user. The first JSON document is the request and the second is the response. This endpoint is called \texttt{notifications}, and the Consumer interacts by creating new notification objects. When the Consumer wishes to notify a user, it is conceptually creating a new notification object, thereby abstracting the concept of performing an action with creating an object.

An important concept of this example is that the business-logic of determining which method of contacting a user is abstracted away from the Consumer entirely, hence the lack of an endpoint for getting the users notification preference. In the background, the Server is taking the appropriate action and hiding it from the Consumer.

In the example response, we do have a \texttt{medium} attribute which represents the method of notifying the user, but that can be omitted depending on if your Consumers need to know this information (perhaps their application has a dashboard which mentions the last time a text/email was sent, and the verbiage should be correct).

\subsubsection{Bad Abstraction}

This example will include numerous shortcomings, and is an easy approach to take by the novice RESTful API architect.

\paragraph{\textbf{Getting User Preference}}

\begin{verbatim}
GET /get_user_preferences/12

{
  "notification_preference": 1
}
\end{verbatim}

This first API Endpoint is called \texttt{get\_user\_preferences}, and is called by passing in the ID of the user whose preference we are looking up (shown here as \texttt{12}). The name of an Endpoint should be a simple noun (or compound nouns). It should not include the action (verb) being performed (in this case \texttt{get}). The reason one should use a simple noun is because this removes ambiguity and tells the Consumer what the ID represents. Does the \texttt{12} represent user \texttt{12}? Or perhaps some user-preference concept which might not correlate 1:1 to a user object?

Another problem with this example is the response contains the integer \texttt{1}. Internally to this fictional service there are some constants where \texttt{1} refers to sending a text, and \texttt{2} refers to sending an email. Even if these values are disclosed in the API documentation, a third-party developer is not going to remember what they represent. Each time they look up what the values mean they are losing productivity.

Yet another problem is that there's an API Endpoint dedicated specifically to getting a user preference. In general, you want to reduce the number of Endpoints in your API, and focus on making each one serve the Consumer better. Data like this could have been merged with another endpoint for getting a user object, for example.

\paragraph{\textbf{Sending a Text or Email (Two Endpoints)}}

\begin{verbatim}
POST /users/12/send_{medium}

{
  "message": "Hello World",
  "sent": "2013-01-06 21:02:00"
}
\end{verbatim}

These second and third endpoints (assuming \texttt{\{medium\}} can represent \texttt{email} or \texttt{text}) have the same problem as the previous endpoint wherein the action is part of the URL (in this case \texttt{send}). These endpoints don't represent data, as the previous one did, they specifically represent an action. Building APIs with these actionable endpoints is a common mistake among developers intending to build a RESTful API.

Another issue with this example is that the business-logic for determining which method of notification to use is left for the Consumer to decide! Sure, the Consumer can make a request to get the users preference, but what if they intentionally ignore it? Or suppose the Consumer caches the preference and it becomes outdated? Either way users are bound to get notified in a manner they didn't choose.

Whenever you find yourself creating two endpoints with the same request inputs and response outputs, there may be a problem with abstraction and the two may be better off combined into one endpoint.

Finally, there is no way to look up previous instances of notifications that have been sent to a user. While another Endpoint could have been created for looking up notification logs, it would likely be ad-hoc or inconsistent with existing Endpoints.

\subsection{Real World Examples}

Let's look at some real-world examples of how popular APIs do their data abstraction.

\subsubsection{GitHub: An Ideal Example}

The current GitHub v3 API \cite{GITHUBAPI} is a beautiful example of a properly-abstracted RESTful API. Each of the popular HTTP verbs are used where applicable. Endpoints don't have verbs in the name. Interacting with an endpoint feels much like one is working with a representation of an object, instead of performing actions.

An example of a good endpoint is \texttt{GET /repos/\{user\_id\}/\{repo\_id\}/notifications}. This is obviously the endpoint used for getting a list of notifications of a particular repository. The \texttt{\{user\_id\}/\{repo\_id\}} convention for referring to a repository is one understood by most users of GitHub (repository names aren't globally unique, only unique to a particular user). The only thing that could be improved may be to not shorten \texttt{repositories} to \texttt{repos} and \texttt{organizations} to \texttt{orgs} in the names of endpoints, although \texttt{repo} is well understood.

\subsubsection{Twitter: A Flawed Example}

The current Twitter v1.1 API \cite{TWITTERAPI} has some abstraction shortcomings with their data and business-logic, as far as being a RESTful API is concerned. The API only makes use of the GET and POST methods for interacting with data. Due to this shortcoming, most endpoint names are a pair of noun and verbs.

One such example is \texttt{POST /statuses/destroy/\{status\_id\}}, used for deleting a status. The RESTful version of this endpoint would be \texttt{DELETE /statuses/\{status\_id\}}. Also worth noting is the differentiation of \texttt{POST /statuses/update\_with\_media} and \texttt{POST /statuses/update}. Both of these endpoints are used for creating a new tweet, however the prior allows for the attachment of media. These two endpoints should be combined into a single \texttt{POST /statuses}, with the media related attributes being optional.

These endpoints are also an example of a bad nomenclature. Users of Twitter don't think of using the service as \emph{updating their status}, they think of it as \emph{tweeting}. This is something that may have changed throughout the lifetime of the service, and if so would be a good candidate to change between API versions. The Collection used by the aforementioned Endpoints would therefor be better named \texttt{tweets}.


\section{Anatomy of an HTTP Message}

Since all RESTful API traffic happens over HTTP, let's examine some raw HTTP messages. It's surprising how many developers who have been building websites for years don't know what an HTTP message looks like! When the Consumer sends a request to the Server, it provides a Request-Line and a set of Key/Value pairs called headers. For POST, PUT, and PATCH requests it also provides two newlines and then the the request body. All This information is sent in the same HTTP request (although this request can be broken up into multiple network packets if the message is large enough).

The Server then replies in a similar format, first with a Status-Line and headers, and typically two newlines followed by a body (the body is technically optional, as you'll find out later). HTTP is very much a request/response protocol; there is no \emph{push} support (the Server does not send data to the Consumer unprovoked). To do that you would need to use a different protocol such as Websockets.

\subsection{HTTP Request}

\begin{verbatim}
POST /v1/animal HTTP/1.1
Host: api.example.org
Accept: application/json
Content-Type: application/json
Content-Length: 24

{
  "name": "Gir",
  "animal_type": "12"
}
\end{verbatim}

\subsection{HTTP Response}

\begin{verbatim}
HTTP/1.1 200 OK
Date: Wed, 18 Dec 2013 06:08:22 GMT
Content-Type: application/json
Access-Control-Max-Age: 1728000
Cache-Control: no-cache

{
  "id": "12",
  "created": 1386363036,
  "modified": null,
  "name": "Gir",
  "animal_type": "12"
}
\end{verbatim}

\subsection{Debugging HTTP Traffic}

\href{http://getpostman.com/}{Postman} (a Google Chrome extension) is an excellent tool for interacting with a RESTful API. As seen in Figure ~\ref{fig:postman}, Postman provides a powerful yet easy-to-use interface for building API requests, as well as debugging the content of API responses. It also provides many advanced features regarding authentication (which we'll cover in later chapters).

\begin{figure}[ht!]
\centering
\includegraphics[width=140mm]{images/postman.png}
\caption{Postman Screenshot}
\label{fig:postman}
\end{figure}

While designing and debugging your API you will sometimes need to debug packets at a lower level than HTTP. A powerful tool for doing this is \href{https://www.wireshark.org}{Wireshark}. You will also want to use a web framework and server which allows you to read and change as many headers as possible.

Figure ~\ref{fig:wireshark} is an example of a complex HTTP request from a form submission on a website. Notice all of the data sent back and forth via HTTP headers. The headers passed around by browsers and web servers is often more chaotic and noisy than what an API Consumer and Server will send.

\begin{figure}[ht!]
\centering
\includegraphics[height=100mm]{images/wireshark.png}
\caption{Wireshark Screenshot}
\label{fig:wireshark}
\end{figure}


\section{API Entrypoint}

The root location of your API is important, believe it or not. When a third-party developer (aka \emph{code archaeologist}) inherits a project using your API and needs to build new features, they may not know about your service at all. Perhaps all they know is a list of URLs which the Consumer communicates with. It's important that the root entry point into your API is as simple as possible, as a long complex URL will appear daunting and can turn developers away.

\subsection{Choosing an Entrypoint}

Here are two common URL schemes that developers use when building an API. These are also sometimes called API entry points.

\begin{itemize}
\item \texttt{https://api.example.com/*} -- \emph{Preferred}
\item \texttt{https://example.org/api/*} -- \emph{Security Implications}
\end{itemize}

First, notice the HTTPS prefix. If API communication is sent unencrypted over the Internet, any third-party along the way is able to eavesdrop. This could include reading sensitive API data, and depending on the chosen authentication method, could allow third-parties to make requests on behalf of the user.

If your application is huge, or you anticipate it becoming huge, putting the API on a dedicated subdomain (in this case \texttt{api.}) is a must. This will allow for more scalability options in the future. It can also be useful for controlling what cookie data can be shared between the content website and the API.

If you anticipate your API will never become large, or you want to build a simple application (e.g. you want to host the website AND API from the same framework), or if your API is entirely anonymous or read-only, placing your API beneath a URL segment at the root of the domain (e.g. \texttt{/api/}) will also work, however it is not a great idea. More considerations will need to be made regarding security, and more potential vulnerabilities can arise. For example, if an XSS vulnerability is discovered on the main website, credentials which might not otherwise be exposed can now be hijacked by a devious third-party.

Do not use a different Top Level Domain (TLD) for hosting your API than for hosting your website. This may sound tempting, as your main domain could be \textbf{example.com}, and your API and developer documentation be entirely located on the trendy \textbf{example.io}. However there is no logical relationship between these two domains as an adversary could have purchased \textbf{example.io}, posing as a legitimate counterpart to \textbf{example.com}. Also, the \emph{code archaeologist} might only have knowledge of one domain and not the other. Finally, if you \emph{do} want to share cookies between the two domains (e.g. an authenticated user on \textbf{example.com} can be automatically logged into the developer site) it cannot be done as easily with two separate TLDs than with a subdomain or even a subdirectory.

\subsection{Content Located at the Root}

It's beneficial to Consumers to have content at the root of your API. For example, accessing the root of GitHub's API returns a listing of endpoints. Personally I'm a fan of having the root URL give information which a lost developer would find useful, like how to get to the developer documentation.

Here's a truncated list of the content provided by the \href{https://api.github.com/}{GitHub API Entrypoint}. Notice how it's both easily readable by a human and easily parsable by a machine (using the URI Template \cite{RFC6570} syntax), and as it lists all endpoints, is a comprehensive guide for the lost developer.

\begin{verbatim}
{
  "current_user_url": "https://api.github.com/user",
  "authorizations_url": "https://api.github.com/authorizations",
  "emails_url": "https://api.github.com/user/emails",
  "starred_url": "https://api.github.com/user/starred{/owner}{/repo}",
  ...
}
\end{verbatim}

Information about the currently-authenticated user can also be placed in the root of the API. As an example, either the user ID or URL to the user would make a great candidate. If you take a similar approach to what GitHub does, one of the keys could be \texttt{current\_user}, and the value could be a URL

It may be tempting to create an endpoint called \texttt{/user} or \texttt{/users/me} for accessing information about the current user, but these would contradict the existing RESTful URL patterns the rest of the API adheres to.


\chapter{API Requests}

\section{HTTP Methods}

You probably already know about GET and POST requests. These are the two most commonly used requests when a web browser accesses web-pages and interacts with data. All browsers, including the antiquated Internet Explorer 6 (IE6), are able to generate these two requests.

There are, however, four and a half HTTP Methods that you need to know about when building a RESTful API. I say "and a half" because the PATCH method is very similar to the PUT method, and the functionality of the two are often combined into just PUT by many API Servers.

You've likely heard of the phrase "CRUD" when referring to the seemingly boiler-plate code many web developers need to write when interacting with a database. Some web frameworks will even generate CRUD "Scaffolding" for the developer as a result of running a terminal command. CRUD stands for Create, Read, Update, and Delete, and short of handling any business logic, can be used for handling all data entry.

Here is the list of HTTP Methods, as well as which CRUD operation they represent, and if your service were to represent an extremely simple database, the associated SQL command:

\begin{itemize}
\item \textbf{GET} (Read)
    \begin{itemize}
    \item Retrieve a specific Resource from the Server
    \item Retrieve a Collection of Resources from the Server
    \item Considered \emph{Safe}: this request should not alter server state
    \item Considered \emph{Idempotent}: duplicate subsequent requests should be side-effect free
    \item Corresponds to a SQL SELECT command
    \end{itemize}
\item \textbf{POST} (Create)
    \begin{itemize}
    \item Creates a new Resource on the Server
    \item Corresponds to a SQL INSERT command
    \end{itemize}
\item \textbf{PUT} (Update)
    \begin{itemize}
    \item Updates a Resource on the Server
    \item Provide the entire Resource
    \item Considered \emph{Idempotent}: duplicate subsequent requests should be side-effect free
    \item Corresponds to a SQL UPDATE command, providing null values for missing columns
    \end{itemize}
\item \textbf{PATCH} (Update)
    \begin{itemize}
    \item Updates a Resource on the Server
    \item Provide only changed attributes
    \item Corresponds to a SQL UPDATE command, specifying only columns being updated
    \end{itemize}
\item \textbf{DELETE} (Delete)
    \begin{itemize}
    \item Destroys a Resource on the Server
    \item Considered \emph{Idempotent}: duplicate subsequent requests should be side-effect free
    \item Corresponds to a SQL DELETE command
    \end{itemize}
\end{itemize}

Here are two lesser-known HTTP Methods. Feel free to use them in your API, however they aren't a requirement to good RESTful API design.

\begin{itemize}
\item \textbf{HEAD}
    \begin{itemize}
    \item Retrieve meta data about a Resource (just the headers)
    \item E.g. a hash of the data or when it was last updated
    \item Considered \emph{Safe}: this request should not alter server state
    \item Considered \emph{Idempotent}: duplicate subsequent requests should be side-effect free
    \end{itemize}
\item \textbf{OPTIONS}
    \begin{itemize}
    \item Retrieve information about what the Consumer can do with the Resource
    \item Corresponds to a SQL EXPLAIN statement
    \item Considered \emph{Safe}: this request should not alter server state
    \item Considered \emph{Idempotent}: duplicate subsequent requests should be side-effect free
    \end{itemize}
\end{itemize}

By making use of the HTTP methods, and not using actionable-verbs within the URL itself, a simpler interface is presented to the developer. Instead of wondering which verbs apply to which nouns (do I \texttt{send} or \texttt{mail} an \emph{email}? Do I \texttt{remove} or \texttt{fire} an \emph{employee}?), a disambiguous and consistent convention is provided instead.

Typically, GET requests can be cached (and often are!) Browsers, for example, will cache GET requests (depending on cache headers), and will go as far as alert the user if they attempt to POST data for a second time. A HEAD request is basically a GET without the response body, and can be cached as well.

If you plan on allowing JavaScript Consumers running in web browsers, and making requests from different domains, the OPTIONS method will need to be supported. This is called \href{https://en.wikipedia.org/wiki/Cross-origin_resource_sharing}{Cross Origin Resource Sharing (CORS)}, and is a fairly-new concept. Basically, it provides a set of request and response headers for defining which domains can access data and which HTTP Methods they can utilize.


\section{URL Endpoints}

\begin{figure}[ht!]
\centering
\includegraphics[scale=.5]{images/zoo-relationships.eps}
\caption{Diagram of Relationships}
\label{fig:zoorelationships}
\end{figure}

An Endpoint is a URL within your API which points to a specific Resource or a Collection of Resources. Typically RESTful APIs use a Plural naming convention for Collections.

\subsection{Top-Level Collections}

If you were building a fictional API to represent several different Zoo's, each containing many Animals (with an animal belonging to exactly one Zoo), employees (who can work at multiple Zoos) and keeping track of the species of each animal, you might have the following endpoints:

\begin{itemize}
\item \texttt{https://api.example.com/v1/\textbf{zoos}}
\item \texttt{https://api.example.com/v1/\textbf{animals}}
\item \texttt{https://api.example.com/v1/\textbf{animal\_types}}
\item \texttt{https://api.example.com/v1/\textbf{employees}}
\end{itemize}

Each piece of data separated by a slash is a URL Segment. Try to keep these as simple as possible.

\subsection{Specific Endpoints}

When referring to what each endpoint can do, you'll want to list valid HTTP Method and Endpoint combinations. For example, here's a semi-comprehensive list of actions one can perform with our fictional Zoo-keeping API. Notice that each endpoint is preceded with the HTTP Method, as this is the same notation used within an HTTP Request header.

\begin{itemize}
\item \texttt{GET v1/zoos}: List all Zoos (perhaps just ID and Name, not too much detail)
\item \texttt{POST v1/zoos}: Create a new Zoo
\item \texttt{GET v1/zoos/\{zoo\_id\}}: Retrieve an entire Zoo resource
\item \texttt{PUT v1/zoos/\{zoo\_id\}}: Update a Zoo (entire resource)
\item \texttt{PATCH v1/zoos/\{zoo\_id\}}: Update a Zoo (partial resource)
\item \texttt{DELETE v1/zoos/\{zoo\_id\}}: Delete a Zoo
\item \texttt{GET v1/zoos/\{zoo\_id\}/animals}: Retrieve a listing of Animals (ID and Name)
\item \texttt{GET v1/animals}: List all Animals (ID and Name)
\item \texttt{POST v1/animals}: Create a new Animal
\item \texttt{GET v1/animals/\{animal\_id\}}: Retrieve an Animal resource
\item \texttt{PUT v1/animals/\{animal\_id\}}: Update an Animal (entire resource)
\item \texttt{PATCH v1/animals/\{animal\_id\}}: Update an Animal (partial resource)
\item \texttt{GET v1/animal\_types}: Retrieve a listing (ID and Name) of all Animal Types
\item \texttt{GET v1/animal\_types/\{animaltype\_id\}}: Retrieve an entire Animal Type resource
\item \texttt{GET v1/employees}: Retrieve an entire list of Employees
\item \texttt{GET v1/employees/\{employee\_id\}}: Retrieve a specific Employee
\item \texttt{GET v1/zoos/\{zoo\_id\}/employees}: Retrieve a listing of Employees at this Zoo
\item \texttt{POST v1/employees}: Create a new Employee
\item \texttt{POST v1/zoos/\{zoo\_id\}/employees}: Hire an Employee at a specific Zoo
\item \texttt{DELETE v1/zoos/\{zoo\_id\}/employees/\{employee\_id\}}: Fire an Employee from a Zoo
\end{itemize}

The syntax used to describe these URLs seen above (save for the request method) is called a URI Template and is a human-readable and machine-parsable standard for describing URLs. This is a great way to convey URLs in both your API documentation, as well as the API responses themselves. GitHub makes use of this technique when conveying Hypermedia URLs in their responses.

\begin{quote}
A URI Template is a compact sequence of characters for describing a range of Uniform Resource Identifiers through variable expansion.\cite{RFC6570}
\end{quote}

The Root API URL has been omitted in the above examples for brevity. While this can be fine during communications (or books with lazy authors), in your actual API documentation you should display the full URL to each endpoint (e.g. \texttt{GET /animal\_type/\{animaltype\_id\}}).

Notice how the relationships between data is conveyed, for example the many-to-many relationships between Employees and Zoos. By adding an additional URL segment, one can perform relationship interactions. Of course there is no HTTP verb for "FIRE"-ing an employee, but by performing a DELETE on an Employee located within a Zoo, we're able to achieve the same effect.

Also notice how the listing of Endpoints doesn't include every possible method-to-resource combination. For example a Consumer is unable to POST or DELETE to the \texttt{animal\_types} Endpoints. In this fictional situation, only administrators would be able to add new \texttt{animal\_types} using some mechanism outside of the API.

There's nothing wrong with not supporting every method-to-resource combination, as every facet of data manipulation your service offers doesn't necessarily need to be exposed via API. Just keep in mind though, developers will wonder why certain features aren't available, and they may even attempt to use an undocumented Endpoint (such as DELETE-ing an \texttt{animal\_type}). Know that if the functionality isn't documented, a developer may still discover a feature by brute force.


\section{Filtering Resources}

When a Consumer makes a GET request for a Collection, provide them a list of every single Resource matching the requested criteria even though the list could be quite large. Do your best to minimize arbitrary limitations imposed on Consumers, as these limits make it harder for a third party developer to grok the API. If they request a certain Collection, and iterate over the results, never seeing more than 100 items, it is now their job to determine where this limit is imposed. Is their ORM buggy and limiting items to 100? Is the network chopping up large responses?

Do offer the ability for a Consumer to specify some sort of filtering/limitation of the results. The most important reason for this, as far as the Consumer is concerned, is that the network payload is minimal and the Consumer gets their results back as soon as possible. Another reason for this is the Consumer may be lazy and wants the Server to do filtering and pagination. The not-so-important reason (from the Consumers perspective), yet a great benefit for the Server, is that the response-generation will require less resources.

Filtering is applicable when performing GETs on Collections. Since these are GET requests, filtering information should be passed via URL parameters. Here are some examples of the types of filtering you could conceivably add to your API, and if your API were a simple representation of a Relational Database, the correlating SQL clause.

\begin{itemize}
\item \texttt{?limit=10\&offset=20}: Pagination of results\newline{}(\texttt{LIMIT 20, 10})
\item \texttt{?animal\_type\_id=1}: Filter records which match the following condition\newline{}(\texttt{WHERE animal\_type\_id = 1})
\item \texttt{?sort\_attribute=name,asc}: Sort the results based on the specified attribute\newline{}(\texttt{ORDER BY name ASC})
\end{itemize}

Some filtering can be redundant with endpoint URLs. In the Endpoints section we had a \texttt{GET /zoo/\{zoo\_id\}/animals} Endpoint. This would be the same thing as \texttt{GET /animals?zoo\_id=\{zoo\_id\}}. Dedicated endpoints will make the lives easier of developers. This is especially true with requests you anticipate will be made frequently. In the documentation, mention this redundancy so that developers aren't left wondering what the differences is.

Also, this may go without saying, but whenever filtering or sorting of data happens, be sure to white-list the column names for which the Consumer can filter and sort by. We don't want any database errors being sent to Consumers!


\section{White-Listing Attributes}

Often times, when a Consumer is making a GET request to a specific Resource or Collection of Resources, they do not need all attributes belonging to the resource(s). Having so much data could also be a network bottleneck. Responding with less data can also reduce the overhead on the Server, e.g. it may prevent an unnecessary database \texttt{JOIN}.

Again, since we're dealing with GET requests, you'll want to accept a URL parameter for white-listing parameters. In theory, blacklisting could work as well, but as new attributes appear in Resources (as additions are backwards compatible), the Consumer ends up receiving data it doesn't want.

The parameter name you choose isn't too important. It could be "filter" or the overly-verbose "attribute\_whitelist". Consistency between different Endpoints is what is most important.

The SQL queries are just an over-simplified example of what could be generated if your API represented a simple Relational Database application.

\subsection{Filtered Request}

In this next example, the Consumer has requested a filtered list of attributes. If you make use of an ORM, filtering of this data should be pretty simple, however if you're manually writing SQL queries, more effort will be involved.

\paragraph{\textbf{Request URL}}

\begin{verbatim}
GET http://api.example.org/user/12?whitelist=id,name,email
\end{verbatim}

\paragraph{\textbf{Resulting SQL Query}}

\begin{verbatim}
SELECT id, name, email FROM user WHERE user.id = 12;
\end{verbatim}

\paragraph{\textbf{Response Body}}

\begin{verbatim}
{
  "id": "12",
  "name": "Thomas Hunter II",
  "email": "me@thomashunter.name"
}
\end{verbatim}

\subsection{Unfiltered Request}

In this example request, the default representation of a user resource includes data from two database tables joined together. One of the tables is the obvious \texttt{user} table, and another table contains some textual data related to a user called \texttt{user\_desc}.

\paragraph{\textbf{Request URL}}

\begin{verbatim}
GET http://api.example.org/user/12
\end{verbatim}

\paragraph{\textbf{Resulting SQL Query}}

\begin{verbatim}
SELECT * FROM user LEFT JOIN user_desc ON user.id = user_desc.id WHERE user.id = 12;
\end{verbatim}

\paragraph{\textbf{Response Body}}

\begin{verbatim}
{
  "id": "12",
  "name": "Thomas Hunter II",
  "age": 27,
  "email": "me@thomashunter.name",
  "description": "Blah blah blah blah blah."
}
\end{verbatim}


\section{Body Formats}

A common method for APIs to receive data from third-parties is to accept a JSON document as the body. Assuming your API is providing data to third-parties in the form of JSON, as a consequence those same third-parties should be able to produce JSON documents as well.

There are two common methods a Web Browser will use when send data to a web server. If you're using a common web language/framework for building an API, such as \emph{PHP} or \emph{Express.js} or \emph{Ruby on Rails}, you're already used to consuming data using these two methods. Web Servers (e.g. \emph{Apache} or \emph{NGINX}) abstract the differences of these two methods and will provide your programming language data in one easy to consume manner. The two methods are called Multi-part Form Data (required for file uploads), and URL Form Encoded (most forms use this latter method).

\subsection{JSON}

The JSON document used for the Request can be very similar to the JSON document used for the Response. JSON can specify the type of data (e.g. Integers, Strings, Booleans), while also allowing for hierarchal relationships of data. String data needs to be escaped (e.g. a quote \texttt{"} becomes prefixed with a backslash \texttt{\symbol{92}"}), but this is usually done automatically for the developer.

\begin{verbatim}
POST /v1/animal HTTP/1.1
Host: api.example.org
Accept: application/json
Content-Type: application/json
Content-Length: 24

{
  "name": "Gir",
  "animal_type": "12"
}
\end{verbatim}

\subsection{Form URL Encoded}

This method is used by Websites for accepting simple data forms from a web browser. Data needs to be URL Encoded if they contain any special characters (e.g. a space character becomes \texttt{\%20}). This is automatically taken care of by the browser.

\begin{verbatim}
POST /login HTTP/1.1
Host: example.com
Content-Length: 31
Accept: text/html
Content-Type: application/x-www-form-urlencoded

username=root&password=Zion0101
\end{verbatim}

\subsection{Multi-Part Form Data}

This method is used by Websites for accepting more complex data from a web browser, such as file uploads. No escaping needs to happen, although the boundary is a random string and needs to not be contained within the uploaded file or data.

\begin{verbatim}
POST /file_upload HTTP/1.1
Host: example.com
Content-Length: 275
Accept: text/html
Content-Type: multipart/form-data; boundary=----RANDOM_jDMUxq4Ot5

------RANDOM_jDMUxq4Ot5
Content-Disposition: form-data; name="file"; filename="hello.txt"
Content-Type: application/octet-stream

Hello World
------RANDOM_jDMUxq4Ot5
Content-Disposition: form-data; name="some_checkbox"

on
------RANDOM_jDMUxq4Ot5--
\end{verbatim}


\chapter{API Responses}

\section{HTTP Status Codes}

It is vital that a RESTful API makes use of the proper HTTP Status Codes; they are a standard after all! Various networking equipment is able to read these Status Codes, e.g. load balancers can be configured to avoid sending requests to a web server sending out many 5XX errors. Client libraries understand if a request has succeeded or failed depending on the Status Code.

\begin{quote}
The first line of a Response message is the Status-Line, consisting of the protocol version followed by a numeric status code and its associated textual phrase, with each element separated by SP characters. No CR or LF is allowed except in the final CRLF sequence.\cite{RFC2616}
\end{quote}

Here is an example of what a complete Status-Line looks like:

\begin{verbatim}
HTTP/1.1 204 File Not Found
\end{verbatim}

\subsection{Common API Status Codes}

There are a plethora of HTTP Status Codes \cite{RFC2616} to choose from, however this list should be a good starting point.

\begin{itemize}
\item \texttt{\textbf{200} OK}
    \begin{itemize}
    \item GET / PUT / PATCH  Requests
    \item The Consumer requested data from the Server, and the Server found it for them
    \item The Consumer gave the Server data, and the Server accepted it
    \end{itemize}
\item \texttt{\textbf{201} Created}
    \begin{itemize}
    \item POST Requests
    \item The Consumer gave the Server data, and the Server accepted it
    \end{itemize}
\item \texttt{\textbf{204} No Content}
    \begin{itemize}
    \item DELETE Requests
    \item The Consumer asked the Server to delete a Resource, and the Server deleted it
    \end{itemize}
\item \texttt{\textbf{400} Invalid Request}
    \begin{itemize}
    \item POST / PUT / PATCH Requests
    \item The Consumer gave bad data to the Server, and the Server did nothing with it
    \end{itemize}
\item \texttt{\textbf{404} Not Found}
    \begin{itemize}
    \item All Requests
    \item The Consumer referenced an inexistent Resource or Collection, and the Server did nothing
    \end{itemize}
\item \texttt{\textbf{500} Internal Server Error}
    \begin{itemize}
    \item All Requests
    \item The Server encountered an error, and the Consumer has no knowledge if the request was successful
    \end{itemize}
\end{itemize}

\subsection{Status Code Ranges}

The first digit of the status code is the most significant, and provides a generalization of what the entire code is for.

\subsubsection{1XX - Informational}

The \textbf{1XX} range is reserved for low-level HTTP stuff, and you'll very likely go your entire career without manually sending one of these status codes. An example of this is when upgrading a connection from HTTP to Web Sockets.

\subsubsection{2XX - Successful}

The \textbf{2XX} range is reserved for successful messages where all goes as planned. Do your best to ensure your Server sends as many of these to the Consumer as possible.

\subsubsection{3XX - Redirection}

The \textbf{3XX} range is reserved for traffic redirection. Most APIs do not use these requests, however, the newer Hypermedia style APIs may make more use of these.

\subsubsection{4XX - Client Error}

The \textbf{4XX} range is reserved for responding to errors made by the Consumer, e.g. they're providing bad data or asking for things which don't exist. These requests should be be idempotent, and not change the state of the server.

\subsubsection{5XX - Server Error}

The \textbf{5XX} range is reserved as a response when the Server makes a mistake. Often times, these errors are created by low-level functions even outside of the developers control, and ensure a Consumer gets some sort of response. The Consumer can't possibly know the state of the server when a 5XX response is received, and so these should be avoidable.


\section{Content Types}

Currently, the most \emph{exciting} of APIs provide JSON data from RESTful interfaces. This includes Facebook, Twitter, GitHub, etc. XML appears to have lost the popularity contest a while ago (except in large corporate environments). SOAP, thankfully, is all but dead. We really don't see many APIs providing HTML to be consumed (save for web scrapers).

Figure ~\ref{fig:googletrends} is a Google Trends graph comparing the terms \emph{JSON API}, \emph{XML API}, and \emph{SOAP API}. This should provide a feel for how their popularity has changed over time.

\begin{figure}[ht!]
\centering
\includegraphics[width=140mm]{images/xml-vs-json-vs-soap-google-trends.png}
\caption{Google Trends for JSON API, XML API, and SOAP API}
\label{fig:googletrends}
\end{figure}

Developers using popular languages and frameworks can very likely parse any valid data format you return to them. You can even provide data in any of the aforementioned data formats (not including SOAP) quite easily, if you're building a common response object and swapping serializers. What does matter though, is that you make use of the Accept header when responding with data.

Some people recommend adding a .json, .xml, or .html file extension to the URL (appended to the Endpoint) for specifying the Content Type to be returned. Unfortunately, with the different extensions added, we've now got different URLs for the same Resources.

Use the \texttt{Accept} header, which is built into the HTTP spec for this purpose, and if you can't provide data in a format the Consumer wants, reply with a \texttt{406 Not Acceptable} response.


\section{Anticipated Body Content}

When a Consumer makes a request to the Server, something needs to be returned as a response. Depending on the HTTP Method being used, the response will be different.

\subsubsection{GET /\{collection\}}

When performing a GET request to an entire Collection, the Consumer expects an array of every Resource being returned. In the simplest form of RESTful APIs, this consists of a single JSON array containing a homogeneous list of resources.

\begin{verbatim}
[
  {
    "id": "1",
    "name": "John Smith",
    "created": "2014-01-01 12:00:00",
    "modified": null
  },
  {
    "id": "2",
    "name": "Jane Doe",
    "created": "2014-01-01 12:01:00",
    "modified": null
  }
]
\end{verbatim}

\subsubsection{GET /\{collection\}/\{resource\_id\}}

When performing a GET request for a specific resource, the Consumer is expecting to receive the resource object. In a simple RESTful API, this is just the resource object as a top level JSON object.

\begin{verbatim}
{
  "id": "2",
  "name": "Jane Doe",
  "created": "2014-01-01 12:01:00",
  "modified": null
}
\end{verbatim}

\subsubsection{POST /\{collection\}}

When performing a POST request to a Collection, the Consumer expects the resource it just created to be returned. In the ideal academic REST API, the object being provided is exactly the same as the object being returned. However, there is very important information being returned to the Consumer which it doesn't already know, such as the \texttt{resource\_id} and other calculated attributes such as timestamps.

\begin{verbatim}
{
  "id": "3",
  "name": "Alice Roberts",
  "created": "2014-01-01 12:02:00",
  "modified": null
}
\end{verbatim}

\subsubsection{PUT /\{collection\}/\{resource\_id\}}

The result of a PUT operation is the entirety of the resource that was updated, as the root JSON object.

\begin{verbatim}
{
  "id": "3",
  "name": "Alice Smith",
  "created": "2014-01-01 12:01:00",
  "modified": "2014-01-01 12:03:00"
}
\end{verbatim}

\subsubsection{PATCH /\{collection\}/\{resource\_id\}}

The result of a PATCH request is exactly the same as the result of a PUT operation. Even though the Consumer may have only acted on some of the resource attributes, the entire resource is returned.

\begin{verbatim}
{
  "id": "3",
  "name": "Alicia Smith",
  "created": "2014-01-01 12:01:00",
  "modified": "2014-01-01 12:04:00"
}
\end{verbatim}

\subsubsection{DELETE /\{collection\}/\{resource\_id\}}

This is the easiest of the bodies to deal with. Once a resource is deleted, you simply return an empty document. No need to return information about the deleted resource. As no body is present, omit the \texttt{Content-Type} header.


\section{JSON Attribute Conventions}

JSON, which stands for \emph{JavaScript Object Notation}, is a subset of JavaScript, and was defined for the purpose of building a language-agnostic data interchange format. It fills mostly the same role that XML was designed to fill, except that it has the side effect of being much more compact, easily deserializing into native objects in most languages, and supporting many different data types (XML only really supports strings).

That said, there is still quite a bit of freedom that a developer has for representing data using JSON. This section of the book is designed to give you advice for representing data.

\subsection{Consistency between Resources}

Whenever you represent different Resources within the same Collection, each attribute should remain of the same data type. For example, if one Resource has a \texttt{name} attribute which is a String, you shouldn't in a different Resource represent it as an Integer.

There are some exceptions, such as when a value doesn't exist, it can be represented as a null. In general though keeping the attributes of the same data type will make your API easier for consumers to use, especially those using statically typed languages (JavaScript and PHP are more forgiving than something like C).

\subsection{Booleans}

It may be tempting to name your Booleans with a prefix or postfix to symbolize the purpose of the attribute. Common examples of this would be to prefix the variable with \texttt{is\_} or end it in \texttt{\_flag}.

This really isn't necessary as attribute names are often self-documenting. For example, if there is an attribute on your User resource called \texttt{administrator}, it should be pretty obvious that using a number or a string isn't intended.

Another tip with Booleans is that they should usually be a \emph{positive} or \emph{happy} word, as opposed to their negative counterparts. This will prevent developers form having to figure out a double-negative in their head. For example, use \texttt{enabled} instead of \texttt{disabled}, \texttt{public} instead of \texttt{private}, and even \texttt{keep} instead of \texttt{purge}.

\subsection{Timestamps}

The ideal standard for representing dates is the \textbf{ISO 8601} \cite{ISO8601} standard, and it looks a bit like this:

\begin{verbatim}
"2014-01-10T03:06:17.396Z"
\end{verbatim}

This format is human readable, lacks redundant information, has variable precision (the microseconds on the end is optional), conveys timezone information, and is the most popular of standardized date formats.

JSON is a subset of JavaScript, and JavaScript does have a default format for parsing dates. The format of these dates looks a little something like this:

\begin{verbatim}
"Thu Jan 09 2014 22:06:17 GMT-0500 (EST)"
\end{verbatim}

That format is as verbose as it is ugly. Who cares what day of the week it was?

If you wanted something much more terse, you could represent the date as the number of seconds since the Unix Epoch, and it can be stored as an integer:

\begin{verbatim}
1389323177396
\end{verbatim}

That format, on the other hand, is a bit too terse. As a human looking at it, you have no idea what date and time it represents! Linux and Unix machines and open source languages can parse that format very easily, however developers in Microsoft land will be clueless.

Here is another common date format. This is what happens when a developer takes a TIMESTAMP directly from a SQL database and outputs it into the response:

\begin{verbatim}
"2014-01-10 03:06:17"
\end{verbatim}

The problem with this format is that the Consumer has no idea what timezone the date is in! You may be tempted to use this format, and document the timezone of the server. However, developers will not remember it, and users of their application will wonder why a newly uploaded image has a modified time of five hours and three seconds ago.

\subsection{Resource Identifiers (IDs)}

Whenever communicating IDs, transfer them as a String (even if they are numeric). Everything a Consumer does with an ID is in string form anyway. If they make a request to the Resource, the ID is concatenated with another String and used as a URL. If the ID is logged, it is written as a String on disk. And unless the Consumer is doing some shady scraping of the API, the ID should never have arithmetic performed on it.

Also, if IDs are always sent as a String, deciding to change from a numeric representation to a different format such as a UUID (e.g. \texttt{7d531700-79a5-11e3-979a-a79bcbe406e9}) or a Base62 encoded value (e.g. \texttt{oHg5SJYRHA0}) will result in no changes to the code on the Consumers end.

\subsection{Nulls}

If most Resources have a particular attribute available, and some do not, you should always provide the attribute in the document with a null value, instead of outright removing the attribute.

This will usually make things easier for your Consumers, who won't need to check if a JSON attribute exists before attempting to read it.

\subsection{Arrays}

When representing Resources with attributes which represent an array, you should give the attribute a plural name. This signifies to the developer that they should expect more than one value.

When an Array shouldn't have any entries, you should typically return an array with nothing in it, instead of returning a null. This really depends on the situation though. Empty arrays and a null have two different meanings.

\subsection{Whitespace}

Whitespace, while convenient for a human to read, isn't very beneficial to a Consumer, and incurs some extra networking overhead. It's really up to you to decide if you want to add whitespace to the output.

JSON allows for any number of whitespace between keys and values, but if you are going to add whitespace, use a simple and consistent standard. Two spaces for indentation and a single newline is a common choice.

\section{Error Reporting}

Errors are an inevitability of inter-party communication. Users will fat finger an email address, developers will not read the tiny disclaimer you hid in your API documentation, and a database server will occasionally be unavailable. When this happens, the server will of course return a 4XX or 5XX HTTP Status Code, but the document body itself should have useful information included.

When designing an error object, there isn't a specific standard that you need to follow. The examples that follow aren't a standard themselves, however feel free to use them as a starting point when designing your own error objects. Make sure that there is consistency between endpoints.

There are essentially two classes of errors you can account for. The first one is a simple error, where it is easy to point to a certain attribute as being the problem. Let's refer to these as validation errors. The second class of errors are a bit more complex, and may not be easily interpreted by an API Consumer. Let's call these generic errors.

\subsection{Validation Errors}

When an error happens regarding a malformed attribute, provide the Consumer with a reference to the attribute causing the error, as well as a message about what is wrong. Assuming the Consumer is a providing a UI for the User to input data, they can not only display the message for the user to read, but if you provide the attribute, the Consumer can provide some context for the User.

\paragraph{\textbf{Erroneous Request}}

\begin{verbatim}
PUT /v1/users/1

{
  "name": "Rupert Styx",
  "age": "Twenty Eight"
}
\end{verbatim}

\paragraph{\textbf{Error Response}}

\begin{verbatim}
400 Bad Request

{
  "error_human": "Inputs not formatted as expected",
  "error_code": "invalid_attributes",
  "fields": [
    {
      "error_human": "Age must be a number between 1 and 100",
      "error_code": "integer_validation",
      "field": "age"
    }
  ]
}
\end{verbatim}

\subsection{Generic Errors}

When an error occurs which can't be traced back to a single input attribute being incorrect, you'll want to return a more generic error construct.

\paragraph{\textbf{Request}}

\begin{verbatim}
POST /v1/animals

{
  "name": "Mittens",
  "type": "kitten"
}
\end{verbatim}

\paragraph{\textbf{Error Response}}

\begin{verbatim}
503 Service Unavailable

{
  "error_human": "The Database is currently unavailable.",
  "error_code": "database_unavailable"
}
\end{verbatim}

\subsection{Always Handle Server Errors}

Make sure that you catch all errors your server is capable of producing, and \emph{always} return content to the Consumer in the format they are expecting!

This sounds obvious, but it can actually be a lot harder than you think. In PHP, for example, extra care has to be made to catch all errors. By default, PHP and many other web languages/frameworks will return HTML formatted errors.

Consumers will throw all sorts of broken data your way. Experiment with your Server and see what sort of errors you can get it to produce. Try sending malformed JSON, upload a 100GB file, corrupt the HTTP headers, make 100k concurrent requests, even try removing the underlying code and see how your web server handles it.

\subsection{String-Based Error Codes}

You might have noticed from these examples that the \texttt{error\_code} attributes contain a simple string as a value. In my opinion, there are two types of strings in programming. The first type of string contains human readable text, which includes punctuation and different letter cases and even Unicode symbols. These strings should never be compared. When I program in a language which supports both single or double quotes for strings, I'll surround these in double quotes.

The other types of strings are computer readable strings. These are much simpler, often used for attributes (you wouldn't use "First Name" as a JSON key, would you?!), and should be all lowercase and contain underscores (or, camelCase, if you're one of \emph{those} people). These strings could pass as names of variables in most languages. I'll usually surround these strings in single quotes.

Back to the point of error codes, it is important to provide the Consumer with \emph{both} a computer readable error code, as well as a human readable error message. The code can be looked up and have logic applied to it by the Consumer. The human readable message can change at any point if a translation changes or any type of rewrite happens.

Many APIs I've seen include the use of integer numeric codes. For example, if there was an error with a database transaction being committed, the error code might be \texttt{2091}. A third-party developer working with the API and coming across that error is going to have absolutely no idea what that number means, and will have to go look it up in the API docs. If that message were instead \texttt{database\_transaction\_failure}, the developer is going to have somewhat of a clue as to what just happened and will be able to continue on their way.

The Stripe API \cite[\#Errors]{STRIPEAPI} makes great use of error strings for conveying error codes. One such example is \texttt{expired\_card}, which as a third-party developer, you immediately know that the user-supplied card has expired.


\section{Responses should Resemble Requests}

As a general rule of thumb, a Response resource should closely resemble the equivalent Request resource. This means that the same attribute names and values are used for Requests as well as Responses.

There are of course a few exceptions. A PATCH, for example, only affects a partial document. A POST won't have certain server-calculated attributes (like an ID or a created timestamp). PATCH and PUTs won't have certain read-only attributes (e.g. created and modified times). These differences in attributes should be minimized whenever possible.

Whenever dealing with the values of attributes, they should always be the same format.

\subsection{Acceptable Discrepancy}

In this example, the differences between the Request and the Response documents are minimal. Some of the values are read-only, and calculated on the server (e.g. \texttt{id}, \texttt{modified}, and \texttt{created}). Some of the attributes have default values (e.g. \texttt{enabled}), which is fine too, as long as these are documented.

\paragraph{\textbf{Request}}

\begin{verbatim}
POST /users

{
  "role": "administrator",
  "name": "Rupert Styx"
}
\end{verbatim}

\paragraph{\textbf{Response}}

\begin{verbatim}
{
  "id": "12",
  "role": "administrator",
  "created": "2014-01-15T02:40:46.049Z",
  "modified": null,
  "name": "Rupert Styx",
  "enabled": true
}
\end{verbatim}

\subsection{Avoidable Discrepancy}

In this example, during a POST to the \texttt{users} endpoint, there is a \texttt{role} attribute, which is a string containing possible user roles, such as \texttt{administrator} or \texttt{moderator}. However, in the response, that same data becomes a Boolean of whether or not the user is an administrator. This doubles the amount of attribute names the Consumer needs to keep track of.

\paragraph{\textbf{Request}}

\begin{verbatim}
POST /users

{
  "role": "administrator",
  "name": "Rupert Styx"
}
\end{verbatim}

\paragraph{\textbf{Response}}

\begin{verbatim}
{
  "id": "12",
  "administrator": true,
  "name": "Rupert Styx"
}
\end{verbatim}


\chapter{The API Ecosystem}

\section{API Versioning}

No matter what you are building, no matter how much planning you do beforehand, your core application will change, your company will pivot, your data relationships will alter, and attributes will be added and removed from resources. This is just how software development works, and is especially true if your project is alive and used by many people.

Remember than an API is a published contract between a Server and a Consumer. If you make changes to the API and these changes break backwards compatibility, you will break deployed applications and third party developers will resent you for it. Do it enough, and they will use somebody else's service instead. To ensure your application continually evolves AND you keep developers happy, you need to occasionally introduce new versions of the API while still allowing old versions to function.

As a side note, if you are simply ADDING new features to your API, such as new attributes on a resource (assuming they are not required to be set), or if you are ADDING new Endpoints, you do not need to increment your API version number as these changes do not break backwards compatibility. You will want to update your API Documentation of course.

Over time you can deprecate old versions of the API. To deprecate a feature doesn't mean to shut if off or diminish the quality of it, but to alert developers that the older version will be removed on a specific date and that they should upgrade to a newer version.

\subsection{Requesting a Specific Version}

There are three common methods RESTful APIs use for communicating which version of the API to use.

\subsubsection{Versioning via a URL Segment (Preferred)}

The method which is the easiest for most Consumers to handle to to add a URL segment after the root location of the API and the specific Endpoints. Changing the URL is the easiest thing for developers to do.

The most common argument against this approach is that \texttt{/v1/users} and \texttt{/v2/users} supposedly represent the same data, and using redundant URLs for the same data violates good REST principles. However, the two URLs likely \emph{do not} represent the same data, as one could be abstracting data completely different than the other. There's also no guarantee endpoints will be named the same between versions.

\begin{verbatim}
https://api.example.org/v1/*
\end{verbatim}

It is customary to use the letter \textbf{v} followed by an integer when versioning this way. Due to the nature of APIs, changing versions often is discouraged, and point releases usually aren't needed.

\subsubsection{Versioning via the Accept Header}

One method is to use a custom Accept header, where the Consumer specifies the type of content they are expecting along with the version of the API. This method may be the \emph{purest} as far as REST is concerned.

The Accept header offers a way to specify generic and less generic content types, as well as specifying fall-backs. In the example below, we are requesting a more specific version of JSON, specifically, Version 1 of the API JSON.

\begin{verbatim}
Accept: application/json+v1
\end{verbatim}

\subsubsection{Versioning via a Custom Header}

Another method is to use a custom header. This is quite similar to the method above. Consumers would still use the normal Accept header they've been using.

\begin{verbatim}
X-Api-Version: 1
\end{verbatim}

\section{Authentication and Authorization}

There are two common paradigms in which your API may authorize Consumers. Using the Two-Legged paradigm, there are two parties involved, a Consumer and your Server. In the Three-Legged paradigm, there are three parties involved, a Consumer, your Server, and a User who probably has an account with both services.

In theory, your API could use both methods of authorization for different areas. Also, there may be some sections of the API which can be accessed anonymously and can entirely bypass the authorization process.

\subsection{Two-Legged Authentication}

\begin{figure}[!htb]
\centering
\includegraphics[scale=.6]{images/two-legged.eps}
\caption{Two-Legged Authentication (2LA)}
\label{fig:twolegged}
\end{figure}

As you can tell in Figure \ref{fig:twolegged}, the concept of Two-Legged Authentication (\emph{2LA}) is quite simple. Essentially, the Consumer needs a way to authenticate themselves with the API. Due to the stateless nature of HTTP, this authentication needs to be present with every request.

Modern websites make use of sessions for handling this state, where a session identifier is passed along with every request via a cookie. With an API, you would never require the use of a cookie (they're typically difficult to work with), but the method we will use is essentially the same thing.

\begin{quote}
The introduction of site-wide state information in the form of HTTP cookies is an example of an inappropriate extension to the protocol. Cookie interaction fails to match RESTs application state model, often resulting in confusion for the typical browser application.\cite[Page 145]{ACMV2N2}
\end{quote}

The HTTP protocol conveniently gives us a header called \texttt{Authorization} for passing around this sort of information. While there are many different ways to do API authorization, many of them make use of this header in some manner.

\subsubsection{Basic HTTP Authentication}

\begin{figure}[ht!]
\centering
\includegraphics[width=140mm]{images/basic-http-auth.png}
\caption{Basic HTTP Authentication Dialog in FireFox}
\label{fig:basichhtpauth}
\end{figure}

The "classical" method of performing authentication is called Basic HTTP Auth \cite{RFC2617}, where a User Agent (meaning a web browser or Consumer) first makes a GET request to a protected resource. The Server responds with the \texttt{401 Unauthorized} header, and the User Agent displays a dialog to the user prompting the user for a username and password.

\paragraph{\textbf{First Request}}

\begin{verbatim}
GET /protected HTTP/1.1
Host: www.example.org
Accept: text/html
\end{verbatim}

\paragraph{\textbf{Unauthorized Response}}

\begin{verbatim}
HTTP/1.1 401 Unauthorized
Date: Thu, 9 Jan 2014 23:35:00 GMT
WWW-Authenticate: Basic realm="Example"
\end{verbatim}

At this point, the user either clicks cancel and is taken to a not-very-useful screen and chooses to go somewhere else, or they enter correct credentials and click Authorize. Entering the wrong credentials typically results in the Server sending back the same Unauthorized status.

The credentials the user supplies are transmitted as follows: The username (which cannot contain ":") is concatenated with ":", and then concatenated with the password. This text is then \href{https://en.wikipedia.org/wiki/Base64}{Base64 Encoded} and sent in the Authorization header. As you can probably guess, this is extremely insecure if done over unencrypted HTTP.

\paragraph{\textbf{Authorized Request}}

\begin{verbatim}
GET /protected HTTP/1.1
Host: www.example.org
Accept: text/html
Authorization: Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ==
\end{verbatim}

Finally, the Server begins giving the User Agent the content which the user is expecting. This exact same Authorization header is sent with every subsequent request.

Introducing HTTP Basic Authorization to your API is just as easy, except that instead of having a browser on the other end, it would be a Consumer of your API. The initial unauthorized request wouldn't need to be performed as the Consumer should know that it needs to first be authorized, however, if the Consumer does provide incorrect credentials, the server would still reply with a \texttt{401 Unauthorized} status.

\subsubsection{Alternatives to Basic Auth}

You can invent your own method of auth where you supply the Consumer with a single randomly-generated and impossible-to-guess token, which they simply provide in the Authorization header (this concept is often referred to as an \emph{Auth Token}). Third parties may want the ability to revoke Auth Tokens, and to generate multiple ones for their Application. Make sure that you provide an administration interface so developers can provision and revoke these tokens themselves.

OAuth, which is most commonly used for performing Three-Legged Authentication, provides some methods for doing \emph{2LA}. Typically developers prefer using standards, although \emph{2LA} is usually pretty simple.

\subsection{Three-Legged Authentication}

\begin{figure}[!htb]
\centering
\includegraphics[scale=.6]{images/three-legged.eps}
\caption{Three-Legged Authentication (3LA)}
\label{fig:threelegged}
\end{figure}

As you can see in Figure \ref{fig:threelegged}, Three-Legged Authentication (\emph{3LA}) is a bit more complex. A user likely trusts your application with their username and password, however they don't trust a third-party consumer. The user would also like to one day revoke the third-parties access to their data stored on your Server, without the need to change their username and password.

The complexities of \emph{3LA} is far too intricate to exemplify in this book, so you'll want to read more information from a different source. At a high level, they provide a method for a Consumer to sign requests and to validate Consumers are who they say they are. They also provide a method for users to grant privileges to Consumers to access specific data from your API. Users can revoke permissions from Consumers at any point. Of course, the Server can also revoke a Consumers privileges.

OAuth 2.0 \cite{RFC6749} provides a great way of doing \emph{3LA}. With each Request, you can be sure you know which Consumer is making requests, which User they are making requests on behalf of, and provides a (mostly) standardized way of expiring access or allowing users to revoke access from a Consumer, all without the need for a third-party Consumer to know the users login credentials. OAuth 2.0 is used by 

There is also the older OAuth 1.0a \cite{RFC5849} standard, which solves mostly the same problems. This standard works by requiring a hash of OAuth attributes sent over the wire, which includes concepts such as a timestamp and a nonce. These are common in cryptographic systems for providing security, and are mostly made irrelevant by sending data over HTTPS. Whichever method you ultimately choose, ensure it is trustworthy and well-documented, having many different libraries written for the languages and platforms which your Consumers will likely be using.

\begin{quote}
I can honestly tell you that OAuth 1.0a, while it is the most secure of the options, is quite difficult to implement. While maintaining an OAuth 1.0a provider, I was surprised by the number of developers who had to implement their own library since one didn't already exist for their language. After spending many hours debugging cryptic "invalid signature" errors, I must recommend you choose an alternative.
\end{quote}

\subsection{Real-World Support}

Choosing which authentication mechanism to use for your service may be made easier by looking at what other services use, and the reasoning by choosing each method.

\begin{itemize}
\item \textbf{Twitter}: OAuth 1.0a, \href{https://dev.twitter.com/docs/oauth/xauth}{xAuth} (proprietary), OAuth 2.0
    \begin{itemize}
        \item OAuth 1.0a is kept around to support legacy Consumers
        \item xAuth was created to bring some OAuth 2.0 features to OAuth 1.0a (e.g. desktop login)
    \end{itemize}
\item \href{http://developer.github.com/v3/#authentication}{\textbf{GitHub}}: OAuth 2.0, Basic Auth
    \begin{itemize}
        \item Basic Auth will leak user credentials to third-parties
        \item Basic Auth likely chosen for developers testing their own apps
        \item Github users \emph{are} developers after all
    \end{itemize}
\item \href{http://documentation.mailgun.com/quickstart.html#authentication}{\textbf{Mailgun}}: Basic Auth
    \begin{itemize}
        \item Mailgun is purely \emph{2LA}, so Basic Auth is a fine choice
        \item Using Basic Auth makes it easier for novice developers to test the API
    \end{itemize}
\item \href{https://developers.facebook.com/docs/reference/dialogs/oauth/}{\textbf{Facebook Graph}}: OAuth 2.0
    \begin{itemize}
        \item The userbase is Facebooks greatest asset, and is definitely a \emph{3LA} servce
        \item Facebook is a big target for hackers
    \end{itemize}
\item \href{https://www.dropbox.com/developers/core/docs}{\textbf{Dropbox}}: OAuth 1.0a, OAuth 2.0
    \begin{itemize}
        \item OAuth 1.0a is for supporting legacy Consumers
        \item OAuth 2.0 is the preferred method for authenticating with Dropbox
    \end{itemize}
\end{itemize}


\section{Consumer Permissions}

Permissions are a way of specifying which Consumers have access to what data, and more specifically, how they are allowed to manipulate said data.

When dealing with \emph{2LA}, the process for deciding Permissions is likely to be handled very simply. For example, if your Service is owned by Company X, and this company trusts Company Y with certain features of the API, X will probably manually assign Y with more permissive permissions. However, Company Z, which is otherwise unknown to X, will get the default, less permissive permissions. Or, perhaps additional permissions can be earned by paying a fee or mailing in a photo ID.

Regarding \emph{3LA}, the Consumer needs to be able to specify which resources belonging to the user they would like to interact with. When the user authorizes the Consumer, the user is usually prompted with a GUI to review the permissions, perhaps make a decision or two, and either allow or deny access. You've very likely seen these permission prompts with services like Twitter (such as Figure ~\ref{fig:twitteroauth}), Facebook, LinkedIn, etc.

Some services will allow a User to disable permissions (older versions of Facebook allowed this), other services will require the permissions to be accepted or denied outright. You can choose whatever approach you'd like with your service. Do keep in mind that Consumers which don't anticipate the permissions changing can break in weird ways.

\begin{figure}[ht!]
\centering
\includegraphics[width=120mm]{images/permissions-twitter.png}
\caption{Twitter OAuth Permissions}
\label{fig:twitteroauth}
\end{figure}

\subsection{Example Permissions Object}

Specifying permissions will vary depending on the authorization mechanism your API implements. With OAuth 1.0a a standard didn't exist as part of the spec. The Server can accept an additional parameter called \texttt{scope} (or whatever you choose) during the Request Token generation phase. This parameter could be a JSON object which represent the permissions the Consumer is trying to be granted. By passing this parameter during the authorization step, a Consumer is able to get per-user permissions.

The following permissions object could represent a common social media website. It represents a Consumer which wishes to get information about a users profile as well as make changes to their profile, send the user emails using the service (although not have access to their email address), and finally retrieve the list of the users friends, add new friends, and remove existing friends.

\begin{verbatim}
{
  "profile": [ "read", "write" ],
  "email": [ "send" ],
  "friends": [ "read", "add", "remove" ]
}
\end{verbatim}

When the user authenticates the Consumer, they would see a list of each of the operations the Consumer is asking for. Some of them which grant destructive or powerful capabilities should probably be highlighted, such as adding and removing friends, and updating the users profile.

\subsection{Real World Example}

Here's another example taken from \href{https://coinbase.com/docs/api/authentication#permissions}{Coinbase} \cite{COINBASE}. They adhere to the OAuth 2.0 spec for sending permissions and use a simple list of keywords representing permissions spearated by spaces (which are encoded as \texttt{+} symbols). This request would allow the consumer to \texttt{buy}, \texttt{sell}, \texttt{send}, and \texttt{request} Bitcoins on behalf of the authenticated user.

\begin{verbatim}
https://coinbase.com/oauth/authorize?response_type=code
    &client_id=YOUR_CLIENT_ID&redirect_uri=YOUR_CALLBACK_URL
    &scope=buy+sell+send+request
\end{verbatim}

\subsection{Default Consumer Permissions}

At the point in time which a Consumer registers their application with your Server, assuming permissions will need to be the same for every user of their application, they can specify the permissions once. Ideally, your Server could even allow for both of these mechanisms to work in parallel (accepting preset permissions, to be overwritten by optional per-user permissions). This gives developers the greatest amount of control and convenience.


\section{API Analytics}

Keep track of the version/endpoints of your API being used by Consumers. This can be as simple as incrementing an integer in a database each time a request is made. There are many reasons that keeping track of API Analytics is a good idea, for example, the most commonly used API calls should have their database queries optimized to reduce server load.

For the purpose of building an API which developers will love, the most important thing is that when you do deprecate a version of your API, you can actually contact developers using deprecated features. This is the perfect way to remind them to upgrade before you kill the old version.

If you can, try to keep track of a matrix worth of analytics data. For example, which endpoint is used, in which version of the API, and by which Consumer (and perhaps even which User for \emph{3LA}). If you get angry users telling you that certain features are broken, having this information will be very helpful for diagnosing problems.


\section{Documentation}

Writing good documentation is vital to the success of any API. If consumers don't know how to use an API, they won't use an API.

Make your Documentation available to the public, and especially search engines. Keeping documentation hidden behind a login prompt will have a few detriments. Developers won't be able to use Google for finding documentation, developers will be annoyed when they have to login and re-navigate to the docs, and potential developers won't know the capabilities of your API before deciding if they should sign up.

Do not use automatic documentation generators! But if you do, at least make sure you're cleaning up the output and making it presentable. Generated docs can be useful with libraries where code connects directly to it, or even Remote Procedure Call (RPC) style APIs where the code and API are closely connected. However, automatically-generated documentation typically sucks.

Do not truncate example request and response bodies, instead show the whole thing. Even specify which HTTP headers the Consumer should expect to see. Make use of a syntax highlighter in your documentation as color-highlighted JSON is much easier to parse with human eyes.

Document expected response codes and possible error messages for each endpoint, and what could have gone wrong to cause those errors to happen. Keep a place where all anticipated error codes can be looked up as well.

Make sure your documentation can be printed. CSS is a powerful thing; don't be afraid to hide that sidebar when the docs are printed. Even if nobody ever prints a physical copy, you'd be surprised at how many developers like to print to PDF for offline reading.

Documentation can either be split into many different pages, or kept on one long page. If you are keeping documentation on one long page, break it up into sections with anchor tags and provide a Table of Contents so that developers can link to parts and share links with others.


\section{Convenience of Developer Experimentation}

Providing convenient tools will allow developers to quickly test API commands without having paste sample code into their own application. This allows them to get familiar with your API much quicker.

\subsection{Web-Based Developer Console}

A Web-based developer console, e.g. Figure \ref{fig:devconsole}, will allow developers to test API commands without ever leaving the documentation website.

\begin{figure}[ht!]
\centering
\includegraphics[width=140mm]{images/api-console.png}
\caption{Example API Console}
\label{fig:devconsole}
\end{figure}

Your server will need a website where third-party developers can register their applications, get authentication credentials, read documentation, etc. This is a great place to put the API Console.

Ensure using the Developer Console is easy and efficient for the developer to use. Perhaps even provide them with a default user account which resets every hour using a CRON job. Maybe by clicking a single button their application listing, the credentials are stored automatically and the developer can begin making API calls on behalf of their application.

If possible use GET parameters for the Developer Console HTML form when it submits. This way, from within the documentation, a developer could click a link describing an API Endpoint and immediately be taken to the console where the Endpoint is executed.

\subsection{Providing cURL Commands}

cURL is a command-line utility available for many platforms (it even comes shipped with many Linux distributions). You may have installed cURL as a dependency for another project which deals with making HTTP requests.

Services such as \href{http://documentation.mailgun.com/api_reference.html}{Mailgun}, \href{https://stripe.com/docs/connect/oauth}{Stripe}, etc., provide sample cURL queries. When doing \emph{2LA}, sample queries are very easy to execute (3LA is often more difficult due to the required steps beforehand).

\subsubsection{Example cURL Command}

This is the example cURL command displayed on the Mailgun homepage \cite{MAILGUNHOME}. The provided API key is even functional, so by pasting this command into a terminal, a developer can immediately make real API calls.

\begin{verbatim}
curl -s --user 'api:key-3ax6xnjp29jd6fds4gc373sgvjxteol0' \
    https://api.mailgun.net/v2/samples.mailgun.org/messages \
    -F from='Excited User <excited@samples.mailgun.org>' \
    -F to='devs@mailgun.net' \
    -F subject='Hello' \
    -F text='Testing some Mailgun awesomeness!'
\end{verbatim}


\chapter{Beyond REST}

\section{Hypermedia APIs: REST Evolved}

Hypermedia APIs seem to be the future of RESTful API design. They're a pretty amazing concept, going "back to the roots" of how HTTP and HTML was intended to work.

When working with non-Hypermedia RESTful APIs, the URL Endpoints are part of the contract between the API and the Consumer. These Endpoints must be known by the Consumer ahead of time, and changing them means the Consumer is no longer able to communicate with the API as intended.

API Consumers are of course far from being the only user agent making HTTP requests on the Internet. Humans, with their web browsers, are the most common user agent making HTTP requests. Humans, however, are NOT locked into this predefined Endpoint URL contract that RESTful APIs are.

What makes humans so special? Well, we're able to read content, click links for headings which look interesting, and in general explore a website and interpret content to get to where we want to go. If a URL changes, we're not affected (unless, that is, they bookmarked a page, in which case they go to the homepage and find a new route to their beloved article).

The Hypermedia API concept works the same way a human would. Requesting the Root of the API returns a listing of URLs which point perhaps to each collection of information, and describing each collection in a way which the Consumer can understand. Providing IDs for each resource isn't important as long as a URL to the resource is provided.

With the Consumer of a Hypermedia API crawling links and gathering information, URLs are always up-to-date within responses and do not need to be known as part of a contract. If a URL is ever cached, and a subsequent request returns a 404, the Consumer can simply go back to the root and discover the content again.

When retrieving a list of Resources within a Collection, an attribute containing a complete URL for the individual Resources are returned. When performing a POST/PATCH/PUT, the response can be a 3XX redirect to the complete resource.

JSON doesn't quite give us the semantics we need for specifying which attributes are URLs, nor how URLs relate to the current document. HTML, as you can probably guess, does provide this information! We may very well see our APIs come full circle and return back to consuming HTML. Considering how far we've come with CSS, one day it may be common practice for APIs and Websites to use the exact same URLs and HTML content.

Imagine a tool on the internet that you want to use. Could be Google Calendar, or Meetup, or Facebook Events. Also imagine that you want to use other tools too, like email or instant messengers. Normally, integrations between tools are only convenient if you're dealing with a massive suite of tools, such as what is offered by Microsoft or Google. As an example, Google Mail integrates very tightly with Google Calendar and Google+ to provide a seamless user experience.

Now, imagine that these disparate tools created by different companies can work with each other as tightly as these massive suites of tools. Often times when a company builds a single product it is better than the equivalent component of a larger suite. This combination of specific, well-built tools working seamlessly with other services becomes the best of both worlds! The process could work automatically, with the different services automatically discovering each other and configuring themselves to play nicely. This is a future offered by hypermedia APIs.

\subsection{ATOM: An Early Hypermedia API}

\href{https://en.wikipedia.org/wiki/Atom\_\%28standard\%29}{ATOM} \cite{HYPERATOM}, a relative to RSS, is likely one of the first mainstream Hypermedia APIs (except for HTML itself). ATOM is valid XML, and therefore is very easy to parse. Links to other documents use a \texttt{link} tag, and specify both the URL (using a \texttt{href} attribute), as well as the documents relation to the current document (using the \texttt{rel} attribute).

\begin{verbatim}
<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Example Feed</title>
  <subtitle>A subtitle.</subtitle>
  <link href="http://example.org/feed/" rel="self" />
  <link href="http://example.org/" />
  <id>urn:uuid:60a76c80-d399-11d9-b91C-0003939e0af6</id>
  <updated>2003-12-13T18:30:02Z</updated>
  <entry>
    <title>Atom-Powered Robots Run Amok</title>
    <link href="http://example.org/2003/12/13/atom03" />
    <link rel="alternate" type="text/html"
        href="http://example.org/2003/12/13/atom03.html"/>
    <link rel="edit" href="http://example.org/2003/12/13/atom03/edit"/>
    <id>urn:uuid:1225c695-cfb8-4ebb-aaaa-80da344efa6a</id>
    <updated>2003-12-13T18:30:02Z</updated>
    <summary>Some text.</summary>
      <author>
        <name>John Doe</name>
        <email>johndoe@example.com</email>
      </author>
  </entry>
</feed>
\end{verbatim}


\section{Response Document Standards}

When responding with a document representing a Collection, it is usually adequate to return a top-level array containing each resource object. Likewise, when responding with a document representing a resource, simply returning a top-level object containing the resource is good-enough.

However, there are some standards which smart people have developed for encapsulating these items in a standardized envelope to help give the Consumer some context when parsing the responses.

For example, if making a filtered request limiting the Collection to containing only 10 Resources, how do you let the Consumer know how many total records exist? If an error occurs, sure you reply with a 4XX or 5XX HTTP Status, but what about an object in the body representing an error? These different response document standards provide a standardized method for returning this meta data.

\subsection{JSON Schema}

\href{http://json-schema.org/}{JSON Schema} \cite{JSONSCHEMA} provides a method for describing the attributes provided by an APIs endpoints. This description is written in JSON in such a way as to be both human-readable and easy to work with programmatically. Using JSON Schema, one could easily automate data validation and generation of CRUD forms.

\subsubsection{Example JSON Schema Document}

\begin{verbatim}
{
  "title": "Example Schema",
  "type": "object",
  "properties": {
    "firstName": {
      "type": "string"
    },
    "lastName": {
      "type": "string"
    },
    "age": {
      "description": "Age in years",
      "type": "integer",
      "minimum": 0
    }
  },
  "required": ["firstName", "lastName"]
}
\end{verbatim}

\subsection{JSON API}

The \href{http://jsonapi.org/}{JSON API} \cite{JSONAPI} spec provided a standardized format for structuring response documents by introducing some reserved words which have special meanings (e.g. \texttt{id} must be used for identifying a resource, a convention we've been otherwise following).

An interesting feature of JSON API is that is also provides a method for returning not only a requested resource but also other resources which it depends on, as if anticipating what the Consumer would want.

\subsubsection{Example JSON API Document}

\begin{verbatim}
{
  "links": {
    "posts.author": {
      "href": "http://example.com/people/{posts.author}",
      "type": "people"
    },
    "posts.comments": {
      "href": "http://example.com/comments/{posts.comments}",
      "type": "comments"
    }
  },
  "posts": [{
    "id": "1",
    "title": "Rails is Omakase",
    "links": {
      "author": "9",
      "comments": [ "1", "2" ]
    }
  }],
  "linked": {
    "people": [{
      "id": "9",
      "name": "@d2h"
    }],
    "comments": [{
      "id": "1",
      "body": "Mmmmmakase"
    }, {
      "id": "2",
      "body": "I prefer unagi"
    }]
  }
}
\end{verbatim}

\subsection{Siren}

The \href{http://sirenspec.org}{Siren Hypermedia} \cite{SIREN} spec provides a standard method for representing resources and what actions can be performed on said resources. It is a Hypermedia API so the URL for performing an action or linking to a Resource is provided in the document.

\subsubsection{Example Siren Document}

\begin{verbatim}
{
  "class": [ "order" ],
  "properties": { 
      "orderNumber": 42, 
      "itemCount": 3,
      "status": "pending"
  },
  "entities": [
    { 
      "class": [ "items", "collection" ], 
      "rel": [ "http://x.io/rels/order-items" ], 
      "href": "http://api.x.io/orders/42/items"
    },
    {
      "class": [ "info", "customer" ],
      "rel": [ "http://x.io/rels/customer" ], 
      "properties": { 
        "customerId": "pj123",
        "name": "Peter Joseph"
      },
      "links": [
        {"rel":["self"],"href":"http://api.x.io/customers/pj123"}
      ]
    }
  ],
  "actions": [
    {
      "name": "add-item",
      "title": "Add Item",
      "method": "POST",
      "href": "http://api.x.io/orders/42/items",
      "type": "application/x-www-form-urlencoded",
      "fields": [
        { "name": "orderNumber", "type": "hidden", "value":"42"},
        { "name": "productCode", "type": "text" },
        { "name": "quantity", "type": "number" }
      ]
    }
  ],
  "links": [
    { "rel": [ "self" ], "href": "http://api.x.io/orders/42" },
    { "rel": [ "previous" ], "href": "http://api.x.io/orders/41" },
    { "rel": [ "next" ], "href": "http://api.x.io/orders/43" }
  ]
}
\end{verbatim}


\section{Alternatives to REST}

\subsection{JSON RPC}

\href{http://www.jsonrpc.org/specification}{JSON RPC} \cite{JSONRPC} is a relatively popular alternative to REST for exposing functionality over a network. Whereas REST is required to be used via HTTP, JSON RPC doesn't have a protocol requirement. It can be sent over sockets, be used with Inter Process Communication (IPC), and yes, it can function over HTTP.

Unlike REST which requires an abstraction of Server business-logic and data into simple objects which can be acted upon using CRUD, JSON RPC calls will typically map to existing functions within your application.

When a client makes a call using JSON RPC, they specify the name of a function to execute, as well as arguments to the function. Arguments can be in the form of either ordered parameters (using a JSON Array), or named parameters (using a JSON Object).

The important part of the specification is the envelope which the data being sent through adheres to. The concept of a URL doesn't really exist (if you're using JSON RPC over HTTP, there's probably a single URL all requests are sent through, and each request is likely sent as a POST).

JSON RPC is mostly useful for situations where you don't have an HTTP server, for example multiplayer games or embedded systems or simple communication applications. If you already have an HTTP Server for your product or service, REST is likely the correct solution for you.

When dealing with HTTP, every Request and Response is guaranteed to be paired together correctly. Due to the asynchronous nature of sockets and other such communication protocols, Requests need to provide a unique ID value, and the corresponding Response needs to provide the same ID.

\subsubsection{Example JSON RPC Request}

\begin{verbatim}
{"jsonrpc": "2.0", "method": "subtract", "params": [42, 23], "id": 1}
\end{verbatim}

\subsubsection{Example JSON RPC Response}

\begin{verbatim}
{"jsonrpc": "2.0", "result": 19, "id": 1}
\end{verbatim}


\subsection{SOAP}

\href{https://en.wikipedia.org/wiki/SOAP}{Simple Object Access Protocol (SOAP)} \cite{SOAP} is a term you've very likely heard of, and even if you've never used it, you'll likely have strong negative opinions of it (and if you have used it, perhaps even stronger). The word \emph{Simple} in the acronym is a sort of a misnomer due to the complexity and verbosity of using SOAP.

SOAP is a sort of successor to an older technology called \href{https://en.wikipedia.org/wiki/XML-RPC}{XML RPC}. As you've probably guessed, XML RPC is similar to JSON RPC, as both are forms of Remote Procedure Call protocols.

SOAP is useful for describing services exposed over the network, and is transport agnostic just like JSON RPC, although most implementations use it over HTTP. Partly due to the waning popularity of XML in comparison to JSON, SOAP is often looked down upon due to the verbosity and the bulkiness of document sizes.

SOAP is mostly used in larger corporate environments.

\subsubsection{Example SOAP Request}

\begin{verbatim}
<?xml version="1.0"?>
<soap:Envelope
    xmlns:soap="http://www.w3.org/2003/05/soap-envelope">
  <soap:Header>
  </soap:Header>
  <soap:Body>
    <m:GetStockPrice xmlns:m="http://www.example.org/stock">
      <m:StockName>IBM</m:StockName>
    </m:GetStockPrice>
  </soap:Body>
</soap:Envelope>
\end{verbatim}

\bibliographystyle{unsrt}
\bibliography{bibliography}

\end{document}
